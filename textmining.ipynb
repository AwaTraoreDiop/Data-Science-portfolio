{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np \n",
    "import nest_asyncio #to avoid runtime issue\n",
    "nest_asyncio.apply()\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') #contents a pre-trained Punkt tokenizer\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "import re \n",
    "import random \n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import twint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from unidecode import unidecode\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. SCRAPPING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('IPython')\n",
    "\n",
    "#c= twint.Config()\n",
    "#c.Pandas = True #on intègre pandas à twint pour pouvoir générer le dataframe lié\n",
    "#c.Search = \"covid\"\n",
    "#c.Since =\"2021-01-02\"\n",
    "#c.Until =\"2021-12-30\"\n",
    "#c.Store_object = True\n",
    "#c.Lang ='fr'\n",
    "#c.Limit = 300000\n",
    "#c.Store_csv = True\n",
    "#c.Output = \"Tweet-File\"\n",
    "\n",
    "\n",
    "#twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II. ANALYSE DESCRIPTIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(r'\\Text Mining_Adrien\\tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets[tweets['language']=='fr'] #nouveau dataframe avec seulement les tweets en français\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_titles'] = df['hashtags'].apply(lambda x: len(x.split())) #take a look the average number of hashtags in tweets\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1) #removing nan\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = None\n",
    "#pd.options.display.max_colwidth = None\n",
    "\n",
    "corpus = list(df['tweet'])\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "for doc in corpus:\n",
    "    print(f\"Le document {i} est de longueur : {len(doc)}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokeninzination\n",
    "def hashtags(corpus):\n",
    "    i = 1\n",
    "\n",
    "    for doc in corpus:\n",
    "        #liste_mots= word_tokenize(doc)\n",
    "        #liste_mots = re.split('\\W+',doc)\n",
    "        hash= re.findall(r\"#(\\w+)\", doc)\n",
    "        print(f\"Le document {i} contient {len(hash)} mots\")\n",
    "        print(f\"La liste de mots du documents {i} : {hash} \\n\")\n",
    "        i+=1\n",
    "    return hash\n",
    "\n",
    "print(hashtags(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doc.isdigit() in corpus :\n",
    "    print(doc)\n",
    "else : \n",
    "    print(\"Tous les documents sont de type string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = ['a', 'abord', 'absolument', 'afin', 'ah', 'ai', 'aie', 'ailleurs', 'ainsi', 'ait', 'allaient', 'allo', 'allons', \n",
    "             'allô', 'alors', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', 'as', 'assez', 'attendu', 'au', \n",
    "             'aucun', 'aucune', 'aujourd', \"aujourd'hui\", 'aupres', 'auquel', 'aura', 'auraient', 'aurait', 'auront', 'aussi', \n",
    "             'autre', 'autrefois', 'autrement', 'autres', 'autrui', 'aux', 'auxquelles', 'auxquels', 'avaient', 'avais', 'avait', \n",
    "             'avant', 'avec', 'avoir', 'avons', 'ayant', 'bah', 'bas', 'basee', 'bat', 'beau', 'beaucoup', 'bien', 'bigre', 'boum', \n",
    "             'bravo', 'brrr', \"c'\", 'car', 'ce', 'ceci', 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', \n",
    "             'celui', 'celui-ci', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', \n",
    "             'cet', 'cette', 'ceux', 'ceux-ci', 'ceux-là', 'chacun', 'chacune', 'chaque', 'cher', 'chers', 'chez', 'chiche', 'chut', \n",
    "             'chère', 'chères', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième', 'clac', 'clic', 'combien', \n",
    "             'comme', 'comment', 'comparable', 'comparables', 'compris', 'concernant', 'contre', 'couic', 'crac', 'c’', \"d'\", 'da', \n",
    "             'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis', 'dernier', 'derniere', 'derriere', 'derrière', \n",
    "             'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant', 'devers', \n",
    "             'devra', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', \n",
    "             'directe', 'directement', 'dit', 'dite', 'dits', 'divers', 'diverse', 'diverses', 'dix', 'dix-huit', 'dix-neuf', \n",
    "             'dix-sept', 'dixième', 'doit', 'doivent', 'donc', 'dont', 'douze', 'douzième', 'dring', 'du', 'duquel', 'durant', 'dès', \n",
    "             'désormais', 'd’', 'effet', 'egale', 'egalement', 'egales', 'eh', 'elle', 'elle-même', 'elles', 'elles-mêmes', 'en', \n",
    "             'encore', 'enfin', 'entre', 'envers', 'environ', 'es', 'est', 'et', 'etaient', 'etais', 'etait', 'etant', 'etc', 'etre', \n",
    "             'eu', 'euh', 'eux', 'eux-mêmes', 'exactement', 'excepté', 'extenso', 'exterieur', 'fais', 'faisaient', 'faisant', 'fait', \n",
    "             'façon', 'feront', 'fi', 'flac', 'floc', 'font', 'gens', 'ha', 'hein', 'hem', 'hep', 'hi', 'ho', 'holà', 'hop', 'hormis', \n",
    "             'hors', 'hou', 'houp', 'hue', 'hui', 'huit', 'huitième', 'hum', 'hurrah', 'hé', 'hélas', 'i', 'il', 'ils', 'importe', \n",
    "             \"j'\", 'je', 'jusqu', 'jusque', 'juste', 'j’', \"l'\", 'la', 'laisser', 'laquelle', 'las', 'le', 'lequel', 'les', \n",
    "             'lesquelles', 'lesquels', 'leur', 'leurs', 'longtemps', 'lors', 'lorsque', 'lui', 'lui-meme', 'lui-même', 'là', 'lès', 'l’', \n",
    "             \"m'\", 'ma', 'maint', 'maintenant', 'mais', 'malgre', 'malgré', 'maximale', 'me', 'meme', 'memes', 'merci', 'mes', 'mien', 'mienne', \n",
    "             'miennes', 'miens', 'mille', 'mince', 'minimale', 'moi', 'moi-meme', 'moi-même', 'moindres', 'moins', 'mon', \n",
    "             'moyennant', 'même', 'mêmes', 'm’', \"n'\", 'na', 'naturel', 'naturelle', 'naturelles', 'ne', 'neanmoins', 'necessaire', \n",
    "             'necessairement', 'neuf', 'neuvième', 'ni', 'nombreuses', 'nombreux', 'non', 'nos', 'notamment', 'notre', 'nous', 'nous-mêmes', \n",
    "             'nouveau', 'nul', 'néanmoins', 'nôtre', 'nôtres', 'n’', 'o', 'oh', 'ohé', 'ollé', 'olé', 'on', 'ont', 'onze', 'onzième', 'ore', \n",
    "             'ou', 'ouf', 'ouias', 'oust', 'ouste', 'outre', 'ouvert', 'ouverte', 'ouverts', 'où', 'paf', 'pan', 'par', 'parce', 'parfois', \n",
    "             'parle', 'parlent', 'parler', 'parmi', 'parseme', 'partant', 'particulier', 'particulière', 'particulièrement', 'pas', 'passé', \n",
    "             'pendant', 'pense', 'permet', 'personne', 'peu', 'peut', 'peuvent', 'peux', 'pff', 'pfft', 'pfut', 'pif', 'pire', 'plein', 'plouf', \n",
    "             'plus', 'plusieurs', 'plutôt', 'possessif', 'possessifs', 'possible', 'possibles', 'pouah', 'pour', 'pourquoi', 'pourrais', 'pourrait', \n",
    "             'pouvait', 'prealable', 'precisement', 'premier', 'première', 'premièrement', 'pres', 'probable', 'probante', 'procedant', 'proche', \n",
    "             'près', 'psitt', 'pu', 'puis', 'puisque', 'pur', 'pure', \"qu'\", 'quand', 'quant', 'quant-à-soi', 'quanta', 'quarante', 'quatorze', \n",
    "             'quatre', 'quatre-vingt', 'quatrième', 'quatrièmement', 'que', 'quel', 'quelconque', 'quelle', 'quelles', \"quelqu'un\", 'quelque', \n",
    "             'quelques', 'quels', 'qui', 'quiconque', 'quinze', 'quoi', 'quoique', 'qu’', 'rare', 'rarement', 'rares', 'relative', 'relativement', \n",
    "             'remarquable', 'rend', 'rendre', 'restant', 'reste', 'restent', 'restrictif', 'retour', 'revoici', 'revoilà', 'rien', \"s'\", 'sa', \n",
    "             'sacrebleu', 'sait', 'sans', 'sapristi', 'sauf', 'se', 'sein', 'seize', 'selon', 'semblable', 'semblaient', 'semble', 'semblent', \n",
    "             'sent', 'sept', 'septième', 'sera', 'seraient', 'serait', 'seront', 'ses', 'seul', 'seule', 'seulement', 'si', 'sien', 'sienne', \n",
    "             'siennes', 'siens', 'sinon', 'six', 'sixième', 'soi', 'soi-même', 'soit', 'soixante', 'son', 'sont', 'sous', 'souvent', 'specifique', \n",
    "             'specifiques', 'speculatif', 'stop', 'strictement', 'subtiles', 'suffisant', 'suffisante', 'suffit', 'suis', 'suit', 'suivant', \n",
    "             'suivante', 'suivantes', 'suivants', 'suivre', 'superpose', 'sur', 'surtout', 's’', \"t'\", 'ta', 'tac', 'tant', 'tardive', 'te', \n",
    "             'tel', 'telle', 'tellement', 'telles', 'tels', 'tenant', 'tend', 'tenir', 'tente', 'tes', 'tic', 'tien', 'tienne', 'tiennes', \n",
    "             'tiens', 'toc', 'toi', 'toi-même', 'ton', 'touchant', 'toujours', 'tous', 'tout', 'toute', 'toutefois', 'toutes', 'treize', 'trente', \n",
    "             'tres', 'trois', 'troisième', 'troisièmement', 'trop', 'très', 'tsoin', 'tsouin', 'tu', 'té', 't’', 'un', 'une', 'unes', \n",
    "             'uniformement', 'unique', 'uniques', 'uns', 'va', 'vais', 'vas', 'vers', 'via', 'vif', 'vifs', 'vingt', 'vivat', 'vive', 'vives', \n",
    "             'vlan', 'voici', 'voilà', 'vont', 'vos', 'votre', 'vous', 'vous-mêmes', 'vu', 'vé', 'vôtre', 'vôtres', 'zut', 'à', 'â', 'ça', 'ès', \n",
    "             'étaient', 'étais', 'était', 'étant', 'été', 'être', 'ô','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "             'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'qu']\n",
    "\n",
    "stopWords = [unidecode(sw) for sw in stopWords]\n",
    "\n",
    "# Création du stemmer\n",
    "stemmer = SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Creation of  functions for cleanning process-----------------------#\n",
    "\n",
    "#find the hashtags \n",
    "def hashtags(tweet):\n",
    "  hash = re.findall(r\"#(\\w+)\", tweet)\n",
    "  return hash\n",
    "\n",
    "#remove username et @username\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "#remove les links \n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # remove bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "#remove non-ascii chars\n",
    "def non_ascii(s):\n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "#lower case\n",
    "def lower(doc):\n",
    "    return doc.lower()\n",
    "\n",
    "#remove les caractères spéciaux\n",
    "def remove_special_characters(tweet):\n",
    "    # define the pattern to keep\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\»\\s]' \n",
    "    return re.sub(pat, '', tweet)\n",
    "\n",
    "#remove stop words\n",
    "def removeStopWords(str, stopWords):\n",
    "    new_tweet = ' '.join([word for word in str.split() if word not in stopWords]) \n",
    "    return new_tweet\n",
    "\n",
    "#remove email address\n",
    "def email_address(doc):\n",
    "    email = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return email.sub(r'',doc)\n",
    "\n",
    "#remove blank space and special caracters\n",
    "def punct(tweet):\n",
    "    token=RegexpTokenizer(r'\\w+')#regex\n",
    "    tweet = token.tokenize(tweet)\n",
    "    tweet= \" \".join(tweet)\n",
    "    return tweet \n",
    "\n",
    "#remove digits\n",
    "def remove_digits(tweet):\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Test cleaning on a sample-------------------------#\n",
    "\n",
    "sample = \"je n'AIME pas ce ,^ #film$**LO-VE-LY@awatra http:www.awa.com awatra@gmail.com  : ;  ?!   123    ok\"\n",
    "punct(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the functions to the dataframe \n",
    "\n",
    "df['hashtag'] = df.tweet.apply(func = hashtags)\n",
    "\n",
    "#begin cleaning\n",
    "\n",
    "df['new_tweet'] = df.tweet.apply(func = remove_users)\n",
    "df['new_tweet'] = df.tweet.apply(func = remove_links)\n",
    "df['new_tweet'] = df.tweet.apply(func = non_ascii)\n",
    "df['new_tweet'] = df.tweet.apply(func = lower)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = email_address)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = removeStopWords , stopWords=stopWords)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = punct)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = remove_links)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = remove_digits)\n",
    "df['new_tweet'] = df.new_tweet.apply(func = remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_tweet'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV. Normalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french') # Vous initialisez le stemmer en français\n",
    "\n",
    "corpus1 = [\" \".join([stemmer.stem(word) for word in doc.split()]) for doc in df['new_tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V. VISUALISATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['word_titles']].hist(figsize=(12,6), bins=8, xlabelsize=8, ylabelsize=9);\n",
    "plt.title(\"Distribution of number of words in covid hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "\n",
    "corp = [len(doc.split()) for doc in corpus1]\n",
    "corp\n",
    "\n",
    "index=list(range(len(corp)))\n",
    "\n",
    "\n",
    "plot = sns.barplot(x=index,y=corp, data=df)\n",
    "    \n",
    "plt.xlabel(' ')\n",
    "plt.ylabel(\"nb\")\n",
    "plt.title(\"Répartition du nombre de mots par tweet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenisation des articles\n",
    "corpus = [doc for doc in corpus1]\n",
    "list_tokens = ' '.join(corpus).split()\n",
    "\n",
    "# Identification et décompte des bigrams\n",
    "list_bigrams = list(ngrams(list_tokens, 2)) \n",
    "counter = Counter(list_bigrams)\n",
    "\n",
    "# Création d'un dataframe contenant le top 20 bigramme les plus fréquents\n",
    "df_plot = pd.DataFrame(counter.most_common(20))\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plot = sns.barplot(x=df_plot[1], \n",
    "                    y=df_plot[0],\n",
    "                    color='#44546a')\n",
    "\n",
    "plt.xlabel('occurrence')\n",
    "plt.ylabel(\"bigramme\")\n",
    "plt.title(\"Top 20 bigrammes catégorie\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ''.join([word for word in corpus1[0:10000]])\n",
    "all_words \n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title(\"les mots fréquemment utilisés dans les hashtags\", weight='bold', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------most usefull hashtags------------------#\n",
    "text = str(df.new_tweet)\n",
    "wordcloud = WordCloud().generate(text)\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(max_font_size=80, max_words=60, background_color=\"white\").generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI. TOPIC MODELLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(df['new_tweet'].values.astype('U')) # U convert the data set to unicode\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(vectorizer.get_feature_names()) #visualising the number of feature the vectorizer gets\n",
    "print(len(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_mod = NMF(n_components=10,random_state=42) #estimatte the topic model of the dataset \n",
    "nmf_mod.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in enumerate(nmf_mod.components_):\n",
    "  print(f\"The TOP 10 words for Topic#{index}\")\n",
    "  print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = nmf_mod.transform(dtm) #combine topics with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[1].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[1].argmax()#index position of the most representative topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_topiclabels = {0:'vaccin en israel',\n",
    "                   1:'sentiments',\n",
    "                   2:'impressions',\n",
    "                   3:'augmentation contaminations dans le monde',\n",
    "                   4:\"fêtes de fin d'année période covid\",\n",
    "                   5:'nouveaux cas, Bordeaux, Quebec  ',\n",
    "                   6:'colère médecin ouest France',\n",
    "                   7:'tests PCR',\n",
    "                   8:'période de grippe',\n",
    "                   9:'masque obligatoire, omicron, covid'\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hash'] = df_topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_label'] = df['hash'].map(bbc_topiclabels)#each hashtag refers to which the topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.groupby('hash').count()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VII CLUSTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca=PCA(n_components=2).fit(dtm.toarray())\n",
    "X_pca=pca.transform(dtm.toarray())\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1])\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination du nombre optimal de cluster\n",
    "wcss=[]\n",
    "\n",
    "for i in range(1,11):\n",
    "    model= KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    model.fit(X_pca)\n",
    "    wcss.append(model.inertia_)\n",
    "\n",
    "#Graphique coude\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title(\"Methode du coude\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"Within Cluster Sum of Squares\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=4).fit(X_pca)\n",
    "y_pred=kmeans.predict(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[y_pred==0,0],y=X_pca[y_pred==0,1],c='red',alpha=0.6,label=\"cluster 1\")\n",
    "plt.scatter(X_pca[y_pred==1,0],y=X_pca[y_pred==1,1],c='blue',alpha=0.6,label=\"cluster 2\")\n",
    "plt.scatter(X_pca[y_pred==2,0],y=X_pca[y_pred==2,1],c='green',label=\"cluster 3\")\n",
    "plt.scatter(X_pca[y_pred==3,0],y=X_pca[y_pred==3,1],alpha=0.6,label=\"cluster 4\")\n",
    "\n",
    "plt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],s=100,c=\"k\",marker=\"X\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"K-Means Clustering\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc68d57425d2c211813c1802e8f4ac88b9dfbc62652b6e4689d3f95d2aaaab5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
